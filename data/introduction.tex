%! title="Introduction"
%! completion="0 0 100"
%! include=true
%! abstract="These three bars will give the relative completion of each
%! chapter in terms of computation, writing, and post-processing.
%! This chapter will contain a general introduction into the subject of
%! Cosmology and Cosmic Structure Formation, including a treatment
%! of the Zeldovich approximation."
%! partof="mainmatter"

\chapter*{Foreword}
The work presented in this thesis brings together concepts from both astrophysics and applied mathematics. It provides astronomical models with new methods of analysis and furthers the abstract understanding of the origin of structure in the Universe. Mathematicians may find a new and interesting application of their methods and a refreshing combination of ideas. This thesis should be readable by scientists from either discipline.

The first part of the introduction is a general exposure to the topics of this thesis. This is followed by a series of appendices which explain technical details needed to be able to reproduce the presented work; these may be skipped at the expense of understanding the thesis chapters in fullness. The reader is expected to have a basic understanding of our place in the Universe, and a grounding in classical mechanics and fluid dynamics.

\chapter{Introduction}
\begin{aquote}{M. Minnaert --- ``De natuurkunde van \'{}t vrije veld''}
    \selectlanguage{dutch}
    ``Denk niet dat de oneindig verscheiden stemmingen der natuur voor de wetenschappelijke waarnemer iets van hun dichterlijkheid verliezen: door de gewoonte van het opmerken wordt ons schoonheidsgevoel verfijnd, en rijker gekleurd de stemmingsachtergrond waarop zich de afzonderlijke feiten aftekenen. De samenhang tussen de gebeurtenissen, het verband van oorzaak en gevolg tussen de onderdelen van het landschap, maken een harmonisch geheel van wat anders slechts een aaneenschakeling zou zijn van losse beelden.''
    \\*
    \selectlanguage{british}

    \textit{``Don't think that the infinite diversity of nature's moods loose any of their poetry to the scientific observer: the practice of noticing refines our sense of beauty and colours the ambience in which remarkable facts emerge.  The relation between events, the connection of cause and effect between the elements of the landscape, make a harmonic unity of what would otherwise be a succession of isolated images.''}
\end{aquote}

\clearpage
\section{Structure in the Universe}
It is the aim of \emph{cosmology} to understand the Universe as a complete entity.  The most fundamental assumption that allows us to undertake such a mission is the \emph{Copernican principle} which states that the Earth occupies no special place in the Universe. This allows us to make observations of our surroundings and, within reason, extrapolate those to the Universe as a whole.
What we see when we make these observations is that the Universe is remarkably \emph{isotropic}, looking the same in every direction. If there is just one other place in the Universe for which this is true, we can conclude that the Universe is \emph{homogeneous}, meaning that it is uniform in composition and character.

However, this is only true on the very largest scales that we can observe. On small scales we see objects like galaxies, stars and planets, entities that are here but not there; we see \emph{structure}. In contrast to the entire Universe, the Earth, the Solar system and the Milky Way are very rich in structure, and the physics needed to describe their origin is comparatively hard. As we go to larger and larger scales, we tend to see less and less structure, the Universe looking more and more similar from one place to the other.  The \emph{cosmic web} is the largest known type of structure, at a scale beyond which the Universe can be considered homogeneous.

\subsubsection{Describing the cosmic web}
Describing the rise of complexity as the first structures form in our infant Universe, is ultimately key to understanding the story of our own origin. It is therefore part of many subfields of astronomy, which each use different approaches in studying the origin of structure because they each face there own challenges.

Even though for example the Solar system is relatively easy to describe in terms of the positions and orbits of planets, we know very little of the origin of the planets. It is clear that planet formation is a complex process where, inside a protoplanetary disk, small clumps of dirt manage to form larger clumps, but the physics is messy and there are a lot of uncertainties. Still, other than the precise classification of Pluto there is little question of what a planet really is, and we can clearly mark one on the sky.  When we study the cosmic web the problem is reverse. We seem to have a good understanding of the origin of the cosmic web, but it is very hard to describe the different elements that play a role in its dynamics.

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{figures/2mrs56}
    \caption{Projected distribution of galaxies on the sky, after \citet{Fairall1998}.  Each dot is a galaxy in the 2MASS redshift survey \citep{Huchra2012}.  The size of the dot is an indication of (K-band) brightness. The Milky Way is shown in blue, the contours follow the all-sky map by \citet{Schlegel1998}.  The galaxies shown are limited to a redshift between $5000$ and $6000\ {\rm km}/{\rm s}$.  The outline of both plots correspond to declination $\delta = 0$, each dotted circle corresponding to $15^{\circ}$ steps.}\label{fig:classicskymap}
\end{sidewaysfigure}

When we look at the cosmic web, we can see a loose network of dense clusters, connected by elongated bridges and flattened sheets of galaxies. Figure~\ref{fig:classicskymap} gives a first impression of what the cosmic web looks like if we plot the the positions of relatively nearby galaxies as we see them on the sky. In this thesis we explore different novel methods by which to study the origin, geometry and evolution of this complex pattern. These methods are based on the \emph{Zeldovich approximation}, which describes the first stages of structure formation in the early universe. Although the Zeldovich approximation, and the related \emph{adhesion model}, have been around for some time, we develop new insights by applying more modern mathematical and computational techniques to the fundamental ideas underlying both models.

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{figures/ps-combo2}
    \caption{Projection of phase-space. The three panels show three successive time steps of a two-dimensional simulation. The properties of the dark matter in the simulation confine it to a thin layer in the four-dimensional phase space. We can derive properties of the dark matter distribution by approximating this layer as a continuous sheet, and projecting to the normal two-dimensional configuration space. In this figure we have visualised one of the extra (Lagrangian) dimension as the third upwards spatial direction. Initially (the left-most panel) this gives a sheet stretched diagonally across phase-space, and the corresponding projection shows little structure. As matter starts to migrate, the sheet starts wrinkle and fold. Where there is a fold, in the projection we see caustics appear. In the end a connected network of nodes and filaments develops.}\label{fig:nbody2phasespace}
    % This caption is full of words that are yet unfamiliar to many readers. I still hope that
    % the gist of this explanation is not lost and that this may actually help to make
    % the reader pre-familiar with these terms (which are explained later on).
\end{sidewaysfigure}

\subsection{This thesis}
The core principles of the techniques and ideas that form the basis of this thesis can be illustrated in a single figure. We exploit the behaviour of \emph{dark matter} when seen in \emph{phase-space}. We can model the dynamics of dark matter as a sheet when it folds in a higher dimensional space.
This principle is illustrated in Figure~\ref{fig:nbody2phasespace}. In Chapter~\ref{chapter:catastrophe} we describe the origin and evolution of the cosmic web in terms of the different possible geometrical configurations in which we can fold the phase-space sheet.
These configurations follow directly from theory of \emph{catastrophes} (Section~\ref{section:catastrophetheory}, developed by the Russian mathematician Vladimir Arnold.

Leading up to this, in Chapter~\ref{chapter:adhesion}, we use \emph{Voronoi tessellations} (Section~\ref{section:computationalgeometry}) to compute the global evolution of the cosmic web (Section~\ref{section:cosmicweb}).
We show how the dual geometries of the Voronoi tessellation and Delaunay triangulation relate to the phase-space geometry of the adhesion model. In Chapter~\ref{chapter:claxon} we extend the application of Voronoi tessellations to more generic models of structure formation using both n-body simulations and the adhesion formalism (Section~\ref{section:modelsofstructureformation}).

The closing Chapter~\ref{chapter:localuniverse} deals with an application of the previously developed methods to models describing the observed structures in the nearby Universe. This results in an atlas of the structures in the closest $90 \Mpc$ around us. \footnote{The unit of $\Mpc$ is the standard unit of distance in cosmology. One $\Mpc$ corresponds to a Hubble recession velocity of $100\ {\rm km/s}$. Taking $h = 0.7$, then $1\ \Mpc \approx 4.40 \times 10^{22} {\rm m}$ or $4.7$~million lightyears.}

% In this thesis we approach the identification of these elements from a theoretical point of view. Starting from the dynamics of matter interacting through the force of gravitation we set out to give an objective method of finding clusters, filaments and walls. Using such a method, we can describe the dynamics of the cosmic web, and the environment in which galaxies form, in a physically meaningful way.

\clearpage
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/cmb-planck}
\caption{The Cosmic Microwave Background radiation. This shows the one part in $10^5$ fluctuations in the temperature of the CMB as measured by the Planck satellite \citep{Planck2013-pp}.  This radiation is the oldest light in the Universe, it has been travelling for 13.7 billion years, before being captured by Planck. These fluctuations are the seeds out of which galaxies form.}\label{fig:cmb}
\end{figure}

\section{The Big Bang}
By now there is little doubt that the Universe as we can observe it had a beginning nearly 14 billion years ago. Our current model of the Big Bang and the subsequent expansion of the Universe has proven to explain many observations and resulted in several predictions that were confirmed with remarkable precision, without entering the still largely metaphysical questions of a more ultimate origin or an existence outside the observable Universe.

The first observations that led to the discovery of the Big Bang consisted of the measurement of both the distance to several nearby galaxies as their velocities in the line of sight usually attributed to \citet{Hubble1929}, though \citet{Lemaitre1927} preceded Hubble in staking this claim.\footnote{Recently \citet{Livio2011} and \citet{VanDenBergh2011} pointed out that \citet{Lemaitre1927} was the first to  publish about this result, having both derived the needed theory from Einsteins theory of GR, and used the necessary observations from Hubble and Str\"omberg to back his claim.  However, the english translation of Lema\^{\i}tre's paper only appeared in 1931. Lema\^{\i}tre concludes that ``L'\'eloignement des n\'ebuleuses extra-galactiques est un effet cosmique d\^u \`a l'expansion de l'espace $\dots$''}

The concept of the \emph{primeval atom}, introduced by Lema\^{\i}tre, led \citet{Alpher1948} and \citet{Gamov1948} to derive the presence of a microwave afterglow remaining from the earliest stages of the \emph{Big Bang}, a term derisively coined by Hoyle, who was a proponent of a \emph{steady state} universe. The observation of the \ac{CMB} radiation \citep{Penzias1965,Dicke1965} was decisive in settling the Big Bang theory as the only tenable theory for the origin of the Universe as we know it.

Subsequent more detailed observations revealed that the \ac{CMB} has an energy spectrum that is extraordinary close to a black-body curve \citep{COBE-FIRAS-1994} of a temperature of $T_{\rm CMB} = 2.7 {\rm K}$. Space is very bright in \ac{CMB} photons, adding up to $\sim 400$ \ac{CMB} photons per ${\rm cm}^3$. This radiation is seen in every direction and the fluctuations in temperature are tiny, only one part in a hundred thousand. The presence of these fluctuations were first predicted from theory \citep{Harrison1970,Peebles1970,Zeldovich1972}, and first observed by the COBE satellite \citep{Smoot1992}. It is in these fluctuations that we find the first seeds of structure (Appendix~\ref{section:gaussianfluctuations}), shown in Figure~\ref{fig:cmb}. These seeds, that we observe from when the Universe was only 380,000 years old, later develop to form galaxies, stars, and every other kind of structure that we see around us.

\subsubsection{Olbers' paradox}
In hindsight, one of the more intriguing pieces of evidence for the Big Bang is the darkness of the night sky, also known as Olbers' paradox\footnote{Although many natural philosophers worried about the night time darkness before Olbers.}. Supposing that the Universe is infinite in both space and time, then where ever we look in the sky, there should at some point be a star. Curiously the paradox was solved by Edgar Allan Poe in his prose poem \emph{Eureka} \citep{Poe1848}.

\begin{aquote}{Edgar Allan Poe}
    Were the succession of stars endless, then the background of the sky would present us a uniform luminosity, like that displayed by the Galaxy --- since there could be absolutely no point, in all that background, at which would not exist a star. The only mode, therefore, in which, under such a state of affairs, we could comprehend the voids which our telescopes find in innumerable directions, would be by supposing the distance of the invisible background so immense that no ray from it has yet been able to reach us at all.
\end{aquote}

To be fair, the inevitability of this conclusion can only be correctly interpreted with more modern theories and observations in hand.

\subsubsection{Dark energy}
% We have already talked about dark matter for a bit. Dark energy is the second big unkown factor in our current understanding of the Big Bang.
From observations of distant supernovae \citep{Riess1998,Perlmutter1999} we know that the expansion of the Universe is accelerating. To account for this we have to assume that the Universe contains some unknown substance with negative pressure. All the different candidates to explain what this substance is are collectively known as \emph{dark energy}. Its presence is not only inferred from supernova data, but also from the amount of clustering observed in the Universe, and the measured flatness of its geometry. Since dark energy exerts negative pressure it cannot form structure, but its effects are felt at the largest scales, where the accelerating expansion of the Universe is stopping the further growth of structure. The precise nature of dark energy is an utter mystery, but all observations are consistent with the case of a non-zero \emph{cosmological constant} $\Lambda$.

\subsubsection{The cosmic pie chart}
The measurements of the \ac{CMB} by \ac{WMAP} \citep{Komatsu2003} and Planck \citep{Planck2013-pp} combined with supernova data by \citet{Perlmutter1999} and \citet{Riess1998}, reveal a $13.7$ billion year old Universe, expanding at a rate of $68.5\ {\rm km}\ {\rm s}^{-1}\ {\rm Mpc}^{-1}$, that consists of $69\%$ dark energy, $27\%$ dark matter, and only $4\%$ normal \emph{baryonic} matter.


\subsection{The expanding Universe}
Now follows a more formal treatment, of the Friedman universe and the meaning of its different constituents including dark energy.

The discovery of the Hubble expansion by Lema\^{\i}tre and Hubble, and the development of modern cosmology are, in a historic perpective, inseparable from the birth of the theory of \emph{general relativity}. It may be considered a historic coincidence that the observations by Hubble came at a time when this (relatively) new and exciting theory of gravity was around.
Newton already knew that a static homogeneous universe\footnote{We write `universe' with lower case when we talk about a possible model of the observable `Universe', which is written with upper case.} would be unstable given his law of gravity. In fact, it is entirely possible to derive the Friedman equations describing the expansion or contraction of an infinite homogeneous sphere, a matter dominated universe, from Newton's laws. What kept scientists from exploring it further was the entrenched belief that the Universe \emph{must} be static on a large scale.

Be that as it may, we need general relativity to give the Friedman model its physical context. It provides us with the geometry of space and time as the fundamental building blocks. This is embodied in the Einstein field equations
\[R_{\mu \nu} - \frac{1}{2} g_{\mu \nu} R + g_{\mu \nu} \Lambda = \frac{8 \pi G}{c^4} T_{\mu \nu}.\]
Here $R_{\mu \nu}$ is the Ricci curvature tensor, $R$ the scalar curvature, $g_{\mu\nu}$ the metric tensor, and $\Lambda$ the cosmological constant. On the right side of the equations we find the constants of nature with the energy content $T_{\mu\nu}$.
These equations tell us how the geometry of space-time is curved by the energy that is present and, at the same time, how matter should move given the curvature of space-time.  The metric gives us a definition of distance. We can only solve this equation if we assume some symmetry and express it in the form of the metric $\d s^2 = g_{\mu\nu}\d x^{\mu}\d x^{\nu}$.

\subsubsection{The Friedman universe}
The symmetry that is assumed to arrive at the Friedman universe is that this universe is both isotropic and homogeneous. This assumption, known as the \emph{cosmological principle}, translates into the Robertson-Walker metric
\[\d s^2 = c^2 \d t^2 - a^2(t) \left(\d r^2 + S_k^2\left(\frac{r}{R_0}\right) \d \Omega^2\right),\]
where
\begin{equation}S_k\left(\frac{r}{R_0}\right) =
    \left\{ \begin{array}{ll} % ChkTex 21
    \sin(r/R_0)\quad & {\rm if}\ k = 1 \\
    r/R_0 & {\rm if}\ k = 0 \\
    \sinh(r/R_0) & {\rm if}\ k = -1 \\
    \end{array} \right. .
\end{equation}
$R_0$ is the curvature radius at present time ($t = t_0$), and $a$ is the Hubble expansion factor. The variants of the metric correspond to \emph{spherical} ($k = 1$), \emph{flat} (euclidian, $k = 0$) and \emph{hyperbolic} ($k = -1$) curvatures of space-time.  Using this metric we can describe the expansion (or contraction) of the universe, parametrised by the Hubble expansion factor $a(t)$, depending on the total energy contents contained in the universe. The Hubble expansion factor $a = 0$ at the time of the big bang, and grows to $a = 1$ at present.

Given this metric one can show that the Einstein field equations reduce to the Friedman equations describing the expansion of the Universe
\begin{shaded*}
    \begin{align}
    {\left(\frac{\dot{a}}{a}\right)}^2 &= \frac{8 \pi G}{3} \rho(t) -
    \frac{kc^2}{R_0^2} \frac{1}{{a(t)}^2} + \frac{\Lambda}{3},\\
    \frac{\ddot{a}}{a} &= - \frac{4 \pi G}{3} \left(\rho(t) +
    \frac{3 P}{c^2}\right) +
    \frac{\Lambda}{3}.
    \label{eq:friedman}
    \end{align}
\end{shaded*}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/expansion}
    \caption{The Hubble expansion. These plots shows the expansion factor as a function of time for the Einstein-de Sitter and concordance universes. The first plot shows how the two models have similar behaviour initially, because the concordance model starts out matter dominated. The second plot shows how the models are normalised to have the same value of $H_0$.} \label{fig:hubble-expansion}
\end{figure}

Here, $\rho(t)$ is the average density of the Universe, $\Lambda$ is the cosmological constant, and $P$ is the pressure exerted by the medium, depending on the contents of the Universe,
\[P = w \rho c^2,\]
where $w$ is the equation-of-state parameter determining how pressure scales with density.

\subsubsection{Energy constituents}
We need to fill the universe with different components to correctly model its dynamics. There is matter, which is assumed to behave \emph{dustlike}, meaning it excerts no significant amount of pressure, having the equation of state parameter $w_m = 0$. There is radiation or more generally speaking, also counting neutrinos, relativistic particles; it has an equation of state with $w_r = 1/3$. This reflects that due to cosmic redshift, photons loose energy and as a result their excerted presure diminishes.

Last item on the list is \emph{dark energy}. Dark energy is needed to account for the accellerating expansion of the universe. It may have any value such that $-1 \le w < -1/3$. All measurements are consistent with $w = -1$ for dark energy, in which case it behaves like the cosmological constant $\Lambda$. These same measurements are also consistent with a value of $w$ being slightly smaller than $-1$, in which case the negative pressure force it represents per unit volume would \emph{grow} as the universe expands. The universe would be heading to a cataclysmic demise where galaxies, planets and eventually even atomic nuclei would be torn appart, an event refered to as the \emph{big rip}. We stick to an interpretation where $w = -1$, merely ending the universe in a \emph{big freeze}.

\subsubsection{Rescaling of parameters}
We'd like to rewrite the Friedman equations in terms of quantities that we can measure. One such quantity is the average recession velocity of galaxies as a function of their distance, the Hubble parameter $H := \dot{a}/a$, which has the units of ${\rm km}\ {\rm s}^{-1}\ {\rm Mpc}^{-1}$.

The other is to rewrite the density in terms of the critical density $\Omega := \rho/\rho_{\rm crit}$. The Friedman equations have a turning point at the critical density
\[\rho_{\rm crit} = \frac{3H^2}{8 \pi G}.\]
The critical density allows us to rephrase the Friedman equations in terms of
\[\Omega := \rho / \rho_{\rm crit},\]
introducing the conversion
\[\frac{3 H^2 \Omega}{2} = 4 \pi G \rho.\]
We can rewrite the first Friedman equation in terms of $H$ and $\Omega$,
\begin{equation}
{\left(\frac{H}{H_0}\right)}^2 = \Omega_{\rm rad} a^{-4} + \Omega_{\rm mat}a^{-3}
+ \Big(1 - \sum\Omega_i\Big)a^{-2} + \Omega_{\Lambda}.
\end{equation}
This last formulation allows us to investigate the past and future fate of the Universe in terms of the different contributions from its constituents. We will next look more closely at two models, the Einstein-de Sitter universe for its theoretical simplicity and the concordance universe as the most realistic option.

%\begin{figure}[t]
%\includegraphics[width=\textwidth]{figures/friedman-phase-mod2.pdf}
%\caption{Scenarios of the Friedman universe. The Friedman equations contain
%some interesting turning points when we vary the most important
%(dimensionless) parameters of the model $\Omega_m$ and $\Omega_{\Lambda}$.
%On the line where $\Omega_{\Lambda} = 0$ the universe expands forever if
%$\Omega < 1$ and recollapses if $\Omega > 1$. In the presence of
%$\Lambda$ this becomes more complicated. On the top left the area that
%is shaded pink indicates a \emph{Big Bounce} scenario. On the border
%of this region is the \emph{loitering} line, in which case there exists
%an unstable equilibrium, which was Einstein's original reason for including
%$\Lambda$ in his equations. On the left, near the matter-only line there
%is the \emph{$\Lambda$-collapse} region. In this case the universe will
%recollapse even in the presence of a small positive $\Lambda$. If
%$\Omega_{\Lambda} < 0$ the universe is always doomed to recollapse.}
%\label{fig:friedman-phase}
%\end{figure}

\subsubsection{Solutions to the Friedman equations}
In the context of this thesis there are two important model universes that we need to consider, the \emph{concordance model} and the \emph{Einstein-de Sitter} model. The latter is a matter-only universe with flat geometry ($\Omega_{\rm m} = 1$). It allows easy analytical solutions to the Friedman equations and as we will see later on, also to the problem of linear structure formation. Solving $a\dot{a}^2 = H_0^2$ by trying $a \sim t^n$ leads to
\[a = {\left(\frac{t}{t_0}\right)}^{2/3}\quad\textrm{and}\quad t_0 = \frac{2}{3 H_0}\]

This solution serves as a benchmark for the more realistic concordance model ($\Omega_{\rm m} = 0.315 \pm 0.017$ and $\Omega_{\Lambda} = 0.685 \pm 0.017$, $w_{\Lambda} = -1$), which is based on the best measurements of the cosmological parameters available.\footnote{At the time of writing, the \citet{Planck2013-pp} results provide the most reliable parameter values.} Solutions to the Friedman equations in the concordance model need to be computed numerically, and are plotted in Figure~\ref{fig:hubble-expansion}.


\section{The cosmic web}\label{section:cosmicweb}
%\subsection{History}
In the 1970s it became clear that galaxies and clusters of galaxies are not randomly distributed. Most notably the Universe shows vast empty regions, called voids \citep{Chincarini1975, Gregory1978, Zeldovich1982, Lapparent1986}. These voids are bound by filaments and walls shaping up a foam-like network outlining the distribution of galaxies. At the nodes of this network we find the largest clustering of galaxies.

Theoretical treatment of what has later become known as \emph{the cosmic web} \citep{Bond1996} finds its roots in the work by \citet{Zeldovich1970}, which has a prominent place in this thesis.
A more detailed understanding of the formation of structure only became available with the advent of the first N-body simulations \citep{Doroshkevich1980, Springel2005} together with extensive \emph{redshift surveys} set out to map the spatial galaxy distribution like \ac{2dFGRS}, \ac{6dFGRS}, \ac{SDSS} and \ac{2MRS} (see Figure~\ref{fig:classicskymap}).
This understanding developed in tandem with precision determination of the cosmological parameters from super novae distance measurements \citep{Riess1998, Perlmutter1999} and observations of the \ac{CMB} \citep{COBE-FIRAS-1994, Ruhl2003, Komatsu2003, Planck2013-pp}.

These and other efforts reveal a Universe where, over long distances, \emph{gravity} is the dominating force.  So much so, that from observations of visible matter (stars, dust and gas), and assuming Einstein's theory of gravity is correct, we cannot account for the amount of gravitational attraction needed to obtain the observed degree of clustering. To solve this issue it is proposed that there must be a large quantity of \emph{dark matter} (for a review, see \citet{Trimble1987} and \citet{Sanders2010}).


\subsection{Dark matter}
This form of matter does not interact with the electromagnetic field but we do notice its gravitational pull on ordinary \emph{baryonic matter}\footnote{Baryons are particles like protons and neutrons, consisting of three quarks. In cosmological context baryonic matter indicates matter that couples to the electromagnetic field, as opposed to dark matter, which does not interact through electromagnetic forces.}. This hypothesis solves issues on a tremendous range of scales. It prevents spiral galaxies from flying apart due to their rotation, binds galaxies into clusters, and at the same time it accounts for the statistics of the fluctuations in the \ac{CMB} temperature and the amount of primordial heavy elements generated during early stages of the big bang \citep{Schramm1998, Olive2000}.

The dark matter hypothesis is not perfect. In all likelihood it requires the introduction of a new fundamental particle. The most popular candidates for such a particle are collectively known as \ac{WIMPs}. Despite many efforts to find such a particle in a laboratory, no candidate has ever been successfully detected. Also from the astrophysical side there are some problems.
The models predict the presence of many small dark matter haloes, as substructure in bigger haloes, and also in voids \citep{Peebles2001}. This low mass population is not reflected in the observations of galaxies. It is unknown whether this points to a problem with the dark matter hypothesis or that our understanding of galaxy formation is inadequate.

Alternative models exist to try to understand the dynamics of galaxies without the use of dark matter. One such model is \ac{MOND} \citep{Milgrom1983,Sanders2002}, a purely empirical model that explains the fast rotation of galaxies by introducing a new regime of physics when gravitation accellerations drop below a certain limit.

Recently the theory of entropic gravity by \citet{Jacobson1995,Verlinde2011,Verlinde2016} showed how \ac{MOND} phenomenology can be arrived at from fundamental principles. Among such phenomena, Verlinde's theory derives the Tully-Fisher scaling relation \citep{Tully1977}, as well as giving predictions for gravitational lensing. This last point also means giving firm predictions in bullet-cluster-like scenarios; the Bullet Cluster currently provides the strongest evidence in favour of dark matter \citep{Clowe2004,Markevitch2004}.

Not withstanding the current possible revolution in our understanding of the physics of gravity, the dark matter hypothesis gives us a good working model for the formation of structure in the Universe. The models and approximations described in this thesis are generic and can be made to fit alternative descriptions of the underlying physics. This is especially true for the case of the adhesion model, since its viscous nature can be argued to give an approximation of the otherwise very complex baryonic physics.

% These attempts are matched with numerous theoretical models by introducing additional fields into general relativity.
% What these models all lack is a consistent cosmological picture, and a theory of large scale structure to match. Though there are some indications that there may be problems with the dark matter hypothesis, at the least it provides us with a good working model, that allows us to make falsifiable predictions on the structure of the cosmic web.

Of all the gravitating matter in the Universe roughly only one-seventh is baryonic. Because dark matter only interacts through the gravitational force it is not influenced by forces of pressure or radiation and hence is easier to model than baryonic matter, which interacts in numerous non-trivial ways.
When we study the formation of structures in the Universe we make the assumption that the baryonic physics does not change the geometry of the resulting matter distribution too much. Then it is enough to study the clustering of dark matter, and find some method to quantify the structures in the dark matter distributions, so that we can compare them to the structures found in the redshift surveys.
A proper understanding of the morphology and dynamics of the cosmic web is needed to study the formation of galaxies. On larger scales information contained in the statistical behaviour of the cosmic web can help us constrain cosmological parameters further and have the potential of discriminating models for modified gravity.


\subsection{Gravitational collapse}
Observations probing the cosmic matter distribution on the largest scales --- the \ac{CMB}, \acp{LRG}, \acp{GRB}, \acp{QSO} and many others --- seem to indicate that the assumption of a homogeneous and isotropic Universe is indeed justified. However on scales of tens to hundreds of megaparsecs, the Universe is permeated with an intricate structure called \emph{the cosmic web}. We may observe matter concentrations to take several distinct morphologies. We find dense clusters of galaxies, that may be connected by elongated filaments, embedded in extended walls. Most of the volume in the Universe is devoid of galaxies. The fact that we find the distribution of galaxies in this distinct pattern can be explained by the \emph{theory of gravitational instability}.

Where the matter is distributed nearly homogeneously just after the Big Bang, slight deviations from the Universal mean density are magnified over a period of billions of years. Where there is a slight excess of matter early on, the corresponding excess gravitational pull makes matter flow inward, increasing the contrast of the primordial matter distribution.

The process of gravitational collapse is \emph{multi-scale}. Objects form on many scales at the same time. In general though, smaller objects tend to form sooner than larger ones. This principle is known as \emph{hierarchical} structure formation.

Initial deviations from sphericity are magnified in the process of gravitational collapse, which can be seen in models describing the evolution of ellipsoidal density peaks \citep{Lin1965,Barrow1981,Bond1996-a,Sheth2001}. Due to this effect, structures that are initially flattened become even more so during their collapse. Within the picture of hierarchical structure formation, at any chosen scale, the flattened structures --- walls --- will be the first to collapse.

\subsubsection{Clusters}
\begin{figure}
    \includegraphics[width=\textwidth]{figures/perseus_cluster_part}
    \caption{The Perseus cluster (A426), optical. This image shows the main concentration of galaxies in the Perseus cluster, spanning $20' \times 12'$ on the sky. The tentacled galaxy is NGC1275 (also known as Perseus A), which is also near the centre of X-ray emission from this cluster. The bright elliptical galaxy near the centre of this image is NGC1272. Credit: R. Jay GaBani.}\label{fig:perseus-optical}
\end{figure}

\begin{figure}
    \includegraphics[width=0.49\textwidth]{figures/perseus-xray-small}
    \includegraphics[width=0.49\textwidth]{figures/perseus-combo}
    \caption{The Perseus cluster (A426) X-ray. This image shows roughly the same region of the sky as the previous Figure~\ref{fig:perseus-optical}. Here we see the X-ray emission emanating from the hot gas that sits in between the galaxies in the cluster (Credit: NASA/CXC/Stanford/\citet{Zhuravleva2014}). On the right we have matched the two images. The brightest part of the X-ray emission comes from the centre of NGC1275, where a super-massive black hole is tearing entire galaxies apart, venting ionised gas back into the intergalactic medium.}\label{fig:perseus-xray}
\end{figure}

Of all the elements in the cosmic web, galaxy clusters seem to be the only objects that we may observe in other ways than looking at the distribution of a sample of galaxies. In addition to containing tens to hundreds of bright galaxies, clusters are places with cores of hot X-ray gas. This makes the cluster visible in X-ray, but more importantly, CMB photons are boosted by inverse Compton scattering off hot electrons in the intra-cluster medium. This \acf{SZe} \citep{Sunyaev1972,Sunyaev1980} leaves a distinct feature in the thermal spectrum of the CMB, making clusters detectable to very high redshifts.

Clusters are often associated with the nodes in the cosmic web. We do need to take care though, to distinguish these nodes in the geometrical sense of the word from the physical objects associated with virialised haloes, X-ray gas and the \acs{SZe}. Both are measurable in today's observations and making this distinction could lead to insights into the interplay between the dynamics of the dark matter and the gas.

\subsubsection{Filaments}
\begin{figure}
    \centering
    % caption does not fit with full textwidth
    \includegraphics[width=0.9\textwidth]{figures/perseus-piscus}
    \caption{The Perseus-Pisces super structure. Perseus-Pisces is one of the bigger conglomerations of galaxies in the local ($z < 0.03$) Universe. Over the centre of this image we can follow a chain of clusters, with branches leading off in all directions, including out to deeper redshifts.}\label{fig:intro-pp}
\end{figure}

Between the nodes in the cosmic web we find elongated chains of galaxies known as filaments. Unlike the clusters, there appears to be no class of physical objects that can be linked to the presence of filaments. A filament has been observed directly in the intergalactic medium \citep{Cantalupo2014}, but this is a single observation at a large redshift of $z \approx 2.3$.

\citet{Dietrich2012} found a dark matter filament between the clusters Abell-222 and Abell-223, by method of weak gravitational lensing. Both this filament and the one observed by \citet{Cantalupo2014} are very large objects, connecting two clusters of galaxies. Filaments have been observed to play an important role in the formation of galaxies on much smaller scales.

There are some cases where galaxies are seen to lie in a filamentary arrangement and showing evidence of concerted behaviour. \citet{Zitrin2008} show one such case, where the galaxies in the filament are seen to have synchronised star formation. This star formation could be triggered by the large scale motions that are common to all the galaxies in this filament. This suggests that the embedding in a filament is an important environmental factor in the formation of these galaxies.

Another such example is given by \citet{Beygu2013}. Here a string of galaxies is found to lie near the center of a larger void. These galaxies seem to have had tidal interaction in the past and are still connected through bridges of neutral hydrogen.

Other studies of a more statistical nature have shown that the spin of galaxies tend to align with their host filament or wall \citep{AragonCalvo2007,Jones2010}. More recently, \citet{Tempel2015a} have shown evidence that satellite galaxies align with the filament to which their host galaxy belongs.

In Figure~\ref{fig:intro-pp} we see the Perseus-Pisces supercluster, as seen through the \ac{2MRS}. This structure is one of the largest conglomerations of galaxies in the nearby Universe. What looks like a string of galaxies may actually be a webbing of multiple filaments contained in a sheet that we see edge-on.


\subsubsection{Walls}
\begin{figure}
    \centering
    % resized to match previous figure
    \includegraphics[width=0.9\textwidth]{figures/coma-wall}
    \caption{The Coma great wall. Over the enormous redshift range of $v = 5000-10000 {\rm km}/{\rm s}$, we can see a planar structure, named after the Coma cluster, the big cluster near the centre of this image. The wall runs from the top of this image at high redshift to lower redshifts on the bottom.}\label{fig:intro-coma}
\end{figure}

Since walls are the first structures to form at any given scale, and small structures form before larger ones, the largest structures that we observe today are walls. One of the first pieces of real evidence of the existence of large flattened structures in the Universe was the discovery of the Local sheet by \citet{Vaucouleurs1953}, and much later the Coma Great Wall in the \acs{CfA} redshift survey \citep{Geller1989}. These are very big structures spanning tens to over a hundred megaparsecs.

In Figure~\ref{fig:intro-coma} we see the \ac{2MRS} galaxies in the direction of Coma over a wide range of redshifts. In the front (in blue) there is a filament leading from the Virgo clusters towards the Coma cluster. Over the entire image we may see a gradient running from blue/green at the bottom towards yellow/red at the top. Embedded in this wall we see a lot of substructure. Both the Coma and Abell-1367 clusters are visible here, as well as several less pronounced features. Towards the top of the image the wall ends in the Hercules supercluster.

\subsubsection{Voids}
Dual to the description of structure in terms of concentrations of matter, we may describe the same structure in terms of the lack of matter. Where over-densities in the initial matter distribution lead to the contraction of surrounding matter, under-dense regions will have a proclivity for expansion. This leads to a view where the Universe is scattered with expanding \emph{voids}, forming visible structures wherever the walls of these cavities meet. This concept has led to some very intriguing approaches to the modelling of the cosmic web, to which we return in the next section.

The first voids were found by \citet{Gregory1978} in their survey around the Coma cluster and the discovery of the Bo\"otes void by \citet{Kirshner1981}.

Voids are the most underdense regions in the Universe. By the lack of matter in voids, objects in voids tend to be less massive, and density contrasts are lower. This might lead us to think that voids are more linear in their behaviour than cluster environments. However, the relative weakness of the dynamical interactions within voids increases the, often non-linear, effects of external forces. There may be tidal forces working from formations of clusters around a void, but more interestingly the emptiness of voids makes them more susceptible to the influence \emph{dark energy} \citep{Biswas2010,Bos2012}.

The same emptiness also offers the relative quiet to study the process of galaxy formation. It is a long standing result that galaxies in dense environments are more massive and redder of colour than \emph{field} galaxies (i.e.\ living  outside the clusters) \citep{Dressler1980}. Void environments provide the extreme end of this \emph{morphology-density relation}. Void galaxies have been studied extensively in the \acf{VGS} \citep{Kreckel2012,Beygu2014}.

\clearpage
\section{Models of structure formation}\label{section:modelsofstructureformation}
As mentioned before, the formation of large-scale structure in the Universe is described by the theory of gravitational collapse. We will now see in a bit more detail how this theory explains the emergence of the complexity found in the cosmic web.

\subsection{Comoving coordinates}
%%~ comoving coordinates
The theory describing cosmic structure formation is complicated by the fact that the stage is set in an expanding Universe. If we have two points in space separated by the \emph{physical} distance $\vec{r}$ now, over the expansion history of the Universe, this vector can be described as
\[\vec{r} = a \vec{x},\]
where $\vec{x}$ is the \emph{comoving} coordinate and $a$ is the \emph{expansion factor}. At the moment of the Big Bang, $a = 0$ the Universe has no size, while today at $t = t_{0}$, we set $a = 1$. We will describe the dynamics of structure formation as it happens on top of the Universal expansion, that is, in co-moving coordinates.

%%~ comoving velocity
If we use co-moving coordinates to describe a location, we also need to use comoving velocity to describe motion. By applying the chain-rule we find
\[\vec{u} := \dot{\vec{r}} = \dot{a}\vec{x} + a\dot{\vec{x}} = \vec{v}_{\rm H} + \vec{v}.\]
This means that the physical velocity between two objects is the sum of the \emph{Hubble flow} $\vec{v}_{\rm H}$ and the comoving velocity $\vec{v} := a\dot{\vec{x}}$.

Note that if the \emph{physical velocity} $\vec{u}$ doesn't change, the comoving velocity $\vec{v}$ drops by $1/a$. This effect is known as \emph{Hubble drag}.

\subsection{N-body simulation}
In the different chapters of this thesis, there are three main models of structure formation that we use. The method approaching a realistic scenario as closely as possible is the \emph{N-body simulation}. This method models the gravitational collapse of dark matter by tracing parcels of matter by representing them as point particles.

Each particle is given a position and momentum, after which we can compute the gravitational pull of each particle on all other particles. In this manner we can sum to find a gravitational acceleration to each particle and compute particle trajectories by iterating in small steps. Evidently this process requires a lot of computing power. Many of the advances in computational astrophysics have been targeted at making N-body experiments more efficient.
% In order to focus on the formation of structure in the Universe and not the dynamics of the Universe itself, this computation is usually performed in \emph{comoving coordinates}, meaning that we divided out the universal and homogeneous Hubble expansion from the equations. In comoving coordinates the volume of our simulation stays constant.

\subsubsection{Zeldovich' Approximation and the Adhesion model}
To be able to understand the first stages of structure formation analytically, \citet{Zeldovich1970} proposed an elegant approximation. Each particle is given an initial comoving velocity, which remains constant in time. When the perturbations in the density are still small the gravitational acceleration of each particle is proportional to the comoving velocity and cancels with the corresponding Hubble drag. The \ac{ZA} is expressed as
\begin{shaded*}
\begin{equation}
\vec{x}(\vec{q}, t) = \vec{q} + \D(t) \vec{u}_{0}(\vec{q}),
\label{eq:izeld}
\end{equation}
\end{shaded*}
where $\vec{x}$ is the current (\emph{Eulerian}) position of a particle and $\vec{q}$ the initial (\emph{Lagrangian}, see Appendix~\ref{section:eulerianlagrangian}) position of that particle. The growth of structure is separated in a time component $\D(t)$ and a spatial component $\vec{u}_{0}(\vec{q})$, where the latter is a function only of the \emph{initial} position. This approximation works surprisingly well in the early stages of gravitational collapse.

However, as particles move unimpeded, at some point in time their trajectories will start to cross. At this time the \ac{ZA} can no longer be considered accurate. To try to remedy this situation we introduce an artifical \emph{viscosity} to prevent particles from crossing orbits. This leads us to complete our triad with the \emph{adhesion model}, which is the subject of Chapter~\ref{chapter:adhesion}.

\subsubsection{Initial conditions}
These different ways to approach the formation of structure need initial conditions to work. We start off with a homogeneous medium of particles, each with a small initial velocity. This velocity field is derived from linear theory of structure formation applied to a perturbed density field. This density field is generated randomly to mimic the conditions in the early Universe.

We provide a more detailed introduction to the basic theory of structure formation and the generation of initial conditions in Appendix~\ref{section:structure-formation}.

\subsubsection{Information and knowledge}
At the danger of diverging from our main subject, we should emphasise the difference between information and knowledge. When we simulate the formation of structure in a computer using an N-body experiment, we compute new information on the positions of particles at a certain time, based on the same information at an earlier time. This information may be considered \emph{knowledge} in the sense that the particle's position and momentum are stored as such in the computer's memory.

% The question of what it means to truly \emph{know} something in this context is the subject of epistemology of computer simulations \citep{Winsberg2015}.

The simulation may also contain information of which we have no knowledge, because this \emph{latent} information has no ontological representation in the computers memory.
Some would say that we don't need knowledge of the cosmic web, as long as we can compare the simulation with observations by creating mock catalogues of galaxies. This presupposes a strictly mechanical view of the scientific method, where knowledge is considered to increase with successful comparison of theory and observation.

In our perspective this is a very bleak view on what it means to do science. We strive to understand the physical processes at a deeper level. To this aim, we \emph{do} want to uncover the latent information and turn it into knowledge. We use mathematical models of structure formation that are inherently more revealing about the nature of the dynamics involved in shaping the cosmic web.

When we run the adhesion model on a set of initial conditions, we get the knowledge on the where and how of the cosmic web with it, because it is inherent to the model. The adhesion model shows us the location and bulk velocity of clusters filaments and walls. In Chapter~\ref{chapter:adhesion} we show how the adhesion model can be computed using techniques from \emph{computational geometry}. These techniques are introduced shortly in Section~\ref{section:computationalgeometry}.

Similarly, our treatment of the \ac{ZA} teaches us something about the way the cosmic web is assembled, just by looking at the initial conditions. To this end we apply the mathematics of \emph{catastrophe theory}, which we introduce in the next section.


\clearpage
\section{Catastrophe theory}\label{section:catastrophetheory}
\begin{figure}
    \centering
    \centerline{\includegraphics[width=\textwidth]{figures/torii}}
    \caption{Caustic formation by projection. The figure illustrates how we may observe different caustics through the projection of a higher dimensional structure. The image shows a torus at different orientations in 3-D space. As we rotate the torus from a face-on to an edge-on view, we observe the changing morphology of the projection of the torus on the 2-D plane at the bottom of the figure panels change their morphology. In three snapshots, we show the metamorphosis of the resulting caustics. Left: instantaneous development of a caustic with two swallow-tail points. Centre: splitting of central caustic into two cusps. Right: as the torus rotates further, the cusp points move to the edge.}\label{fig:torus_intro}
\end{figure}

A central theme in this thesis is the theory of catastrophes. We explained the problem of linking observable objects with the inherent structures of the cosmic web. The core of this problem lies in the transition between a structure defined in terms of a contunious medium (dark matter and/or gas) and physical objects that we can speak about in unambiguous terms. In part this problem is determined by human perception: what do we think a cluster, filament, wall or void should look like? Many previous attempts to identify structures still use the human eye, and our perception of geometry as the ultimate benchmark to gauge the quality of a method.

We would like to move this discussion to the physical domain. The identification of a structure only carries meaning if there is a physical event underlying the structure's formation. For the formation of the cosmic web, this physical event is that of \emph{shell crossing}. Let's consider the behaviour of a pressureless medium (i.e. dark matter) in motion. When matter moves towards some point from two sides, at some point, these \emph{streams} will cross paths. Where this happens the velocity field becomes multi-valued, creating a \emph{multi-stream region}. What happens in more complicated scenarios? To describe all possible ways in which this process can happen we use the mathematical theory of \emph{catastrophes}.

\subsubsection{History}
The original goal of catastrophe theory was to put our intuitive understanding of shape and form into a rigid mathematical frame work, set out in the french mathematician Ren\'e Thom's seminal work ``Stabilit\'e structurelle et morphog\'en\`ese'' (1972). Thom's treatise is written in an obscure mathematical lingo and is deeply philosophical of content at the same time. It was hailed in some circles as the next \emph{Principiae} and generated a burst of publications linking catastrophe theory to many different problems all fields of science. Among other things, catastrophe theory explained the stability of ships and other complex physical systems, but more questionably it was also used to explain face recognition and erratic behaviour in animals and much more. This latter `sociological' part gave catastrophe theory the undeserved name of being pseudo-science, in spite of its underlying mathematical rigour.
The Zeeman catastrophe machine (Appendix~\ref{section:zeemanmachine}) is an example of a relatively simple construction that exhibits catastrophic behaviour.

\subsection{Catastrophes on a torus}
Catastrophes arise naturally in many every day phenomena. Perhaps the simplest example is that of the projection of a torus, as seen in Figure~\ref{fig:torus_intro}. Here we show a torus in three different projections. As we rotate the torus between these projections, the curves that describe the edge of the projection change their shape. Starting face on, the projection of the torus has two edges in the shape of two concentric circles. These edges are known as \emph{caustics}.

As we rotate the torus to an edge-on view, we first see two kinks in the inner caustic (the first image). Rotating further, these kinks split into a distinct swallow-tail pattern; hence this is known as the \emph{swallow-tail catastrophe}. Caustics find a more direct physical manifestation in situations of fluids that exhibit no pressure forces. Examples of such fluids are shown in Figure~\ref{fig:realcaustics}.

\begin{figure}[t]
\centering
    \includegraphics[width=0.66\textwidth]{figures/real-caustics/ngc474_bw.jpg}
    \includegraphics[width=0.33\textwidth]{figures/real-caustics/smoke.jpg}
    \caption{Examples of caustics in the real Universe. Caustics are an ubiquitous feature found in dynamical systems subject to only small amounts of pressure. Here we show two such systems. The galaxy on the left (a deep exposure of NGC 474 \citep{Duc2013}, credits: Duc/Cuillandre/CFHT/Coleum) is composed mostly of stars (and DM), but almost no gas. Gravitational interactions on stars are so small with respect to their distance, that the conceptual `gas of stars' can be taken to be a pressureless medium. The same applies to wafting particles of smoke, as can be seen on the right (credits: Sander van der Wel [\url{http://www.gevoelsfotografie.nl}]). This smoke was created by perturbing a laminar stream of smoke with a screwdriver, before being captured on film.} \label{fig:realcaustics}
\end{figure}

%\subsection{Phase space geometry}
%The \emph{catastrophic} behaviour in Zeeman's machine can be understood by looking at
%the space of all possible physical configurations. We have the two directions in which
%we can move the pointer and the angle of the disc (neglecting its motion) together
%comprising a three-dimensional \emph{phase space}. The positions in this phase space that
%actually solve for the physical system are located on a sheet, also known as the
%catastrophe manifold. If we project this sheet down to the plane of the pointer, the
%caustics appear. It is the principle of this kind of projections from phase-space that we use
%to describe the formation and subsequent evolution of the patterns in the cosmic web.

\subsection{Cosmic catastrophes}
In Chapter~\ref{chapter:catastrophe} we will use ideas from catastrophe theory to identify structures in the cosmic web. More specifically, the branch of catastrophe theory known as \emph{Lagrangian singularity theory}, developed by the Russian mathematician Vladimir Arnold, has a direct application to the formation of structure in the Universe. Through his communications with Zeldovich, Arnold was aware of this application early on, resulting in the publication \citet{Arnold1982} describing the genesis of caustic structures in two dimensions. Arnold followed up with a paper doing the same for the three dimensional case \citep{Arnold1986}.

More recently the interest in the geometry of caustics in the cosmic web was renewed by the emergence of methods of analysing simulations of structure formation in six-dimensional phase-space \citep{Falck2012,Shandarin2012,Abel2012}. The basic principle behind this idea is illustrated in Figure~\ref{fig:nbody2phasespace}. In this case we have a phase space that is spanned not by the position and momentum vectors, but by the current position and the starting position of a particle. The current position is also known as the \emph{Eulerian} position, and the starting position is known as the \emph{Lagrangian} position.

Particles in this phase space are arranged in a sheet configuration known as the Lagrangian sub-manifold or dark-matter phase-space sheet. Initially particles sit at their original Lagrangian position, so that the sheet is completely flat. As soon as we let the particles move, the sheet wrinkles and folds. In the projection to Eulerian (configuration) space we see caustics appear. In Figure~\ref{fig:nbody2phasespace} we used a two dimensional simulation and a four dimensional phase space (of which we can only show three dimensions) to illustrate this principle.

It turns out that, upto a smooth deformation (a homeomorphism), there is only a few stable ways of folding the phase-space sheet. \citet{Arnold1976} developed a classification scheme describing not just these configurations but also how they can morph into each other, depending on your linguistic preference called a \emph{metamorphosis} or \emph{perestroika}. In Chapter~\ref{chapter:catastrophe} we describe these caustic configurations and their transitions in great detail, ever in the context of the formation of the cosmic web.

\subsection{Geometric optics}
\citet{Shandarin1989} proposed to compute the \ac{ZA} with a primitive optical computer. The setup needs a columnated beam of light, a lense polished with potential perturbations, and a projection screen. Light travels towards the lense in parallel rays, meeting it at different angles. Depending on the local gradient of the lenses surface, the rays are deflected in different directions. We see on the screen a pattern of caustics. As we increase the distance from lens to screen, the caustics evolve forward in time. This way we can see the evolution of structures in the 2d \ac{ZA}. It turned out to be impractical to polish a suitable lens. Nowadays it is possible to simulate this entire setup in a computer using a ray-tracer. Figure~\ref{fig:caustic-lens} shows a rendering created with the programs {\small LUX-RENDER} and {\small BLENDER}.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/caustic-lens-bw}
\caption{The setup as it was proposed by \citet{Shandarin1989}, rendered using
{\small BLENDER} and {\small LUXRENDER}. }
\label{fig:caustic-lens}
\end{figure}

\clearpage
\section{Computational Geometry}\label{section:computationalgeometry}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/wrapped-bunnie}
    \caption{The convex hull. On the left of this rendering we see the \emph{Stanford Bunny} with the wire frame of the simplicial complex embellished in black. On the right there is a mirrored version of the same bunny wrapped in its convex hull. The convex hull has no protruding parts. (image rendered with \textsc{Blender})} \label{fig:convexbunny}
\end{figure}

In Chapter~\ref{chapter:adhesion} of this thesis, we will see that the computation of the dynamics and evolution of the cosmic web is closely related to some well known concepts from the field of \emph{computational geometry}. This is a research area where applied mathematics and computer science overlap. Its goal is to find efficient algorithms to find answers to all kinds of questions one could ask about large geometric data sets.

Some key concepts that we will be using are the \emph{Voronoi diagram}, the \emph{Delaunay triangulation} and the \emph{convex hull}. The latter is illustrated in Figure~\ref{fig:convexbunny}. On the left we see a triangulation of a bunny. On the right we show the same (mirrored) bunny now wrapped inside its convex hull. Loosly speaking, the convex hull has no protruding parts.

Two related concepts are those of the Voronoi diagram and the Delaunay triangulation. Given a set of points, the Delaunay triangulation divides space into a set of triangles connecting the points. These triangles are chosen such that their circumcircle is empty of other points. Where points are far appart, triangles tend to be big. Where points are more crowded, the triangles will be smaller. This property has been exploited in in the \ac{DTFE} \citep{Schaap2000}, to compute a density field from a point cloud.

Given a set of points, the Voronoi diagram can be constructed by tessellating space in such a manner that every point is assigned a region that is closest to that point. Where points are far appart, the respective Voronoi cells are big. Where points are more crowded, the Voronoi cells will be smaller. This property has been exploited in the \ac{VTFE} \citep{Neyrinck2005}, to compute a density field from a point cloud.

The Delaunay triangulation and Voronoi diagram are intimately related, being each others \emph{dual}. For each element in the Delaunay triangulation there is an orthogonal element in the Voronoi diagram.

In Chapter~\ref{chapter:adhesion} we will go much further than just computing densities from point clouds. We will show how these geometric concepts can actually be used to \emph{model} the formation of large scale structures. We exploit the existing duality between the Voronoi and Delaunay tessellations to compute properties of the cosmic web. \citet{Weygaert1991} showed that we can understand much of the dynamics of voids using Voronoi tessellations. We go one step further, and show how \emph{weighted} Voronoi tessellations relate to the adhesion model of structure formation.


\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/delaunay}
    \includegraphics[width=0.49\textwidth]{figures/voronoi}
    \caption{The Delaunay triangulation and the Voronoi diagram. On the left is shown the Delaunay triangulation of a random point set. The colour corresponds to the size of the triangles. On the right we see the dual Voronoi diagram of the same point set, here the colours are random. Within each shell the shade becomes darker with distance to the generating point.} \label{fig:delvor}
\end{figure}

\section{Mapping the Universe}
The cosmic web emerges both from observational surveys of large-scale structure and from simulations of the gravitational collapse of dark matter.
Combining these two perspectives is an important question in astronomy: how can we put the observations of galaxies in the context of theoretical models of the cosmic web and vice-versa?

The techniques described in this thesis lay the foundations for a novel and theoretically meaningful way to interpret the observed structure of the Universe.
This combined methodology is explored in Chapter~\ref{chapter:localuniverse}, where we describe the elements of the cosmic web in the local Universe ($z < 0.03$) as they evolve in the adhesion model.

In Figure~\ref{fig:z5060adh} we show the same spherical shell of the Universe as can be seen in Figure~\ref{fig:classicskymap}, now with the walls, as detected in the adhesion model, shown in green.

\begin{sidewaysfigure}
    \includegraphics[width=\textwidth]{figures/small_z50_60}
    \caption{Structures between $50 - 60 \Mpc$. Red diamonds show the galaxies from the \ac{2MRS}. Adhesion walls are show in shades of green. }\label{fig:z5060adh}
\end{sidewaysfigure}

%\clearpage
\begin{subappendices}
%\section{The expanding Universe}\label{section:expanding-universe}
%By now it is no secret that we live in an expanding Universe.
%\subsubsection{$\Lambda$-matter universe}
%Reducing the Friedman equation to the case where we only consider a universe with
%a positive matter content and a cosmological constant, and setting the Hubble parameter
%to unity, gives us the differential equation
%\[\dot{a}^2 = \Omega_m a^{-1} + \Omega_{\Lambda} a^2 + (1 - \Omega_m - \Omega_{\Lambda}),\]
%%where we replaced $\Omega_{\Lambda}$ with $\lambda$ and $\Omega_m$ with $\mu$.
%From this we can see that the universe will arrive at a critical point whenever
%the right-hand-side of this equation vanishes. Multiplying by $a$ once gives us
%a third order polynomial with the standard form
%\[x^3 + \alpha x + \beta = 0. \label{eqn:fm-cusp-form}\]
%Depending on the parameters $\alpha$ and $\beta$ this equation may have one or
%three (possibly coincidental) roots. Which case this is can be determined with the
%discriminant function
%\[D := 4\alpha^3 + 27\beta^2.\]
%If $D > 0$, there is only one root. The universe expands like an Einstein-de Sitter
%universe initially, then slows down a bit due to the gravitational pull of the
%matter content, but eventually $\Lambda$ starts to dominate, and expansion
%accellerates. This is the case for the concordance scenario.
%If $D = 0$ then there are two roots of which one is degenerate. This corresponds
%to the \emph{loitering} solution that Einstein used to make his universe static.
%The loitering solutions are unstable in the cosmic parameters, so a small deviation
%would disrupt this solution. If we pass the loitering line to where $D < 0$ (see figure
%\ref{fig:friedman-phase}), we enter a regime where no big bang ever happened,
%also known as the \emph{big bounce} scenario. Note that in Figure~\ref{fig:friedman-phase}
%there is a second critical line near $\Omega_{\Lambda} = 0$. The behaviour
%here is different from the loitering model. The Friedman equation has a square
%in the left-hand-side, which forces us to also take the accelleration equation
%(\ref{eq:friedman}) into account in order to decide the behaviour near critical points.

%Considering all the flavours of
%universes that general relativity has to offer, we may conclude that, in spite
%of Leibnitz' expressed hope that we live in the best of all possible worlds, we
%do live in one of the more boring ones.
%This little detour on the solutions of the Friedman equations did provide us
%the chance to give a first taste of \emph{catastrophe theory}. Equation
%\ref{eqn:fm-cusp-form} describes the canonical \emph{cusp catastrophe},
%allowing us to distinguish some regions of different qualitative behaviour in a
%system with an otherwise smoothly varying parameters. We will resume to
%describe the cusp catastrophe in Section~\ref{section:ch1-catastrophe}, then
%in the context of multi-flow regions during structure formation.

\clearpage
\section{Structure formation}\label{section:structure-formation}
The Friedman universe is merely the background on which we study the gravitational instability of the inhomogeneities embedded in the otherwise tranquil Universe. We will be describing this process over times spanning from the first few hundred thousand years after the big bang, to today. In the mean time the universe will have expanded by factors of thousands. The small perturbations that will turn out to become galaxies initially expand along with the universe. It is more convenient to look at the development of structures if we divide out the expansion of the Universe. The \emph{comoving coordinate} is defined as
\[\vec{x} := \frac{\vec{r}}{a}.\]
The velocity of a particle then becomes
$\dot{\vec{r}} = \dot{a}\vec{x} + a\dot{\vec{x}}$,
allowing us to define the Hubble flow as $\vec{v}_H := \dot{a}\vec{x}$ and the \emph{comoving velocity} as \[\vec{v} := a\dot{\vec{x}}.\] Similarly the average matter density of the universe drops due to the expansion as $a^{-3}$. To express the evolution of the density perturbations we use the \emph{density constrast}
\[\delta := \frac{\rho}{\rho_u} - 1,\]
where $\rho_u$ is the average density of the universe.
Finally, the gravitational potential has an average contribution of $\Phi_u := - a\ddot{a}x^2/2$ from the Friedman background. Subtracting it from the total potential gives us the \emph{gravitational potential perturbation}
\[\phi := \Phi + \frac{a\ddot{a}x^2}{2}.\]
Using these quantities we can rewrite the known equations of motion in a form more suited to study large scale structure formation.

We start with the basic Newtonian equations for a gravitational system in comoving coordinates, without pressure or viscous forces \citep{Peebles}.
\begin{shaded*}
\begin{align}
\partial_t \delta \big|_x + \frac{1}{a}\gradx\left((1 + \delta) \vec{v}\right) & = 0
%\\ \rho_u \d \vec{q} & = \rho \d \vec{x}
    & \textrm{Continuity equation} \\
\partial_t \vec{v}\big|_x + \frac{1}{a}(\vec{v} \cdot \gradx)\vec{v}
    & = - H\vec{v}-\frac{1}{a}\gradx \phi
%\\ \partial_t (a \vec{v}) \big|_q & = - \gradx \phi
    & \textrm{Euler equation} \\
\frac{1}{a^2}\nabla_{\!\!x}^2 \phi & = 4\pi G \rho_u \delta % ChkTex 21
    & \textrm{Poisson equation}
\end{align}
\end{shaded*}

The continuity equation restates the conservation of mass; as matter flows out of a volume
element, the density must drop and vice versa.


\subsubsection{Log-normal distributions}
Rewriting the continuity equation, changing
the time derivative to Lagrangian form, gives us
\begin{align}
\partial_t \delta \big|_q + \frac{(1 + \delta)}{a}\ \gradx \cdot \vec{v} &= 0\\
\frac{\partial_t (1 + \delta)\big|_q}{1 + \delta} = \partial_t \log(1 + \delta)\big|_q &= -\frac{1}{a}\ \gradx \cdot \vec{v}.
\end{align}
Since $\gradx\cdot\vec{v}$ initially has a normal distribution, this suggests that the density distribution will incline to
log-normal behaviour. This result was first shown by \citet{Coles1991}.

\subsubsection{Lagrangian form of Euler equation}
Using Lagrangian derivatives, we can rewrite the Euler equation into a much simpler and physically more intuitive notation. The Euler equation becomes
\[\partial_t (a \vec{v}) \big|_q = - \gradx \phi,\]
which basically restates Newton's second law. The Poisson equation is very similar to its familiar form in physical coordinates. The average contribution from matter density in the Universe has been subtracted to focus on the development of the \emph{density contrast} $\delta := (\rho - \rho_u)/\rho_u$.

\subsection{Linear theory}
The above set of equations is non-linear. To study their behaviour in the very early universe, we use a linearised set of equations, where $|\delta| \ll 1$, and $\partial_t \vec{v}|_q \sim \partial_t \vec{v}|_x$. This reduces the continuity and Euler equations to
\begin{align}
\partial_t \delta\big|_x &= -\frac{1}{a}\gradx \cdot \vec{v} \\
\partial_t \vec{v}\big|_x &= -H\vec{v} - \frac{1}{a}\gradx\phi
\end{align}

\begin{SCfigure}[1.0]
%\begin{shaded*}
\includegraphics[width=0.5\textwidth]{figures/growth}
\caption{Growing mode solution in Einstein-de Sitter and concordance model.
Notice that in the concordance model linear structure formation eventually stops.}
%\end{shaded*}
\label{fig:growing-mode}
\end{SCfigure}

Taking the gradient of the linearised Euler equation and shuffling terms leads us to
the second order differential equation
\[\ddot{\delta} + 2H \dot{\delta} - \frac{3}{2}\Omega H^2 \delta = 0 \label{eq:linear}\]
This last equation has two solutions,
\[\delta(\vec{x}, t) = \D(t)\Delta_{\mathsmaller{+}}(\vec{x}) + \ D_{\!\mathsmaller{-} }(t)\Delta_{\mathsmaller{-}}(\vec{x}),\]
known as the \emph{growing} and \emph{decaying} modes. The precise solution of Equation~\ref{eq:linear}
depends on $H(t)$, and thus on the background cosmology. In the case of the Einstein-de Sitter universe,
$H = 2/3\ t^{-1}$ and $\Omega = 1$, gives
\[\ddot{\delta} + \frac{4}{3t}\dot{\delta} - \frac{2}{3 t^2}\delta = 0.\]
This is solved by inserting $\delta \sim t^n$, giving
\[
    \D(t) = {\left(\frac{t}{t_0}\right)}^{2/3} = a(t)\quad \textrm{and}\quad D_{\!\mathsmaller{-}}(t) = {\left(\frac{t}{t_0}\right)}^{-1}.
\]
Because the decaying mode is exactly that, decaying, it is usually ignored in further analysis. In the case of a universe containing matter and a cosmological constant we can compute an approximation to the correct solution by numerical integration,
\[\D(t) = H(t) \int \frac{\d t}{a^2 H^2}.\]

In a universe dominated by dark energy with $-1 \le w < -1/3$, there is no growth of structure in the linear regime. This means that, in the concordance universe,  the growth factor $\D$ will converge to a value of $\sim 1.4$.The value of $\D$ is shown as a function of scale-factor in Figure~\ref{fig:growing-mode}. We will soon see that both Eulerian and Lagrangian perturbation theory scale with the growth factor. From this result we can go on to define the linearised gravitational potential
\[\nablax^2 \phi_{\rm lin} = 4\pi G \rho_u \delta_{\rm lin} a^2 = 4\pi G \rho_u \D \delta_0 a^2.\]
%\frac{3H^2 \Omega \delta_{\rm lin}}{2} = \frac{3H^2 \Omega \D \delta_0}{2} = \D \nablax^2 \phi_0

From the linearised continuity equation we can now see that
\[\gradx \cdot \vec{v} = -a \partial_t \delta\big|_x = -a \dot{D} \delta_0\]
and combining with the linearised Poisson equation
\[\vec{v} = -\frac{2 f}{3H\Omega} \frac{\gradx \phi_{\rm lin}}{a},\]
where $f(\Omega)$ is the dimensionless velocity growth factor, or \emph{Peebles-factor},
defined as
\[f := \frac{a \dD}{\dot{a} \D}.\]
The Peebles factor can be approximated by the simple formula $f(\Omega_{\rm m})
\approx \Omega_{\rm m}^{0.6}$.

\clearpage
\section{Gaussian random fields}\label{section:gaussianfluctuations}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/rf}
    \caption{Example of Gaussian random field. This shows a Gaussian random field in 2-d as a height map.} \label{fig:random-field}
\end{figure}

The CMB radiation, shown in Figure~\ref{fig:cmb}, has been measured to follow Gaussian statistics to high precission \citep{Komatsu2003,Planck2013-pp}.  This is the prime reason that we use \acf{GRF} to describe the initial conditions used in our simulations. We use Gaussian statistics to describe the probability that a function has a certain value at some point $\vec{x}$. That is, we have a random function
\[f: \mathbb{R}^n \to \mathbb{R},\]
for which we'd like to draw instances of the entire function. In Figure~\ref{fig:random-field} we show one such instance in $\mathbb{R}^2$ as a hight map. It turns out that it is rather straight forward to generate a \ac{GRF} on a computer, but the concepts surrounding the theoretical background can be tricky to grasp.

\begin{definition}[GRF]
A \ac{GRF} has the property that all its $N$-point distribution functions are (multivariate) Gaussian.
\end{definition}

Suppose we have a continuous density field $f: \mathbb{R}^n \to \mathbb{R}$ and a set of points $\vec{x} = \{\vec{x}_1, \vec{x}_2, \dots\}$, we may then ask questions on the statistics of this field on this point set, $y_i = f(\vec{x}_i)$. In the case of a \ac{GRF}, and a single point $\vec{x}_1$ being probed, the value of $y_1 = f(\vec{x}_1)$ follows a Gaussian distribution,
\[P(y_1) = \frac{1}{\sqrt{2\pi} \sigma_1} \exp\left(-\frac{{(y_1 - \mu_1)}^2}{2 \sigma_1^2}\right).\]
To simplify things, we assume a vanishing mean $\mu = 0$, an assumption that is by definition true for the density perturbation $\delta = (\rho - \bar{\rho})/\bar{\rho}$.

The $N$-point distribution function describes the joint probability of the function $f$ at the $N$ points $\vec{x_1}, \vec{x_2}, \dots \vec{x_N}$ having the values $\vec{y} = y_1,y_2, \dots y_N$, and is given by the multivariate Gaussian
\[P(\vec{y}) = \frac{1}{\sqrt{{(2\pi)}^N \det M}}\exp\left(-\frac{1}{2} \vec{y}^{\dagger}M^{-1}\vec{y}\right), \label{eq:npoint}\]
where $M$ is the covariance matrix $M_{ij} = \langle y_i^{\star} y_j\rangle$.

The multivariate Gaussian has the defining property that any linear combination of any or all of its components follows a univariate Gaussian distribution. This means that if an $N$-point function is Gaussian, all lower point statistics are Gaussian.
As a consequence, we can write the entire $N$-point function as a product of $N(N-1)/2$ two-point functions. In fact it is enough to know the two-point function to describe the complete statistics of a Gaussian random field. In the case of $N=2$, we can write $M$ in the form
\[M_2 = \begin{pmatrix}
    \sigma^2 & \xi \\
    \xi & \sigma^2
\end{pmatrix},\]
where $\xi$ is the \emph{two-point correlation function},
\[\xi(\vec{x}, \vec{x} + \vec{r}) := \langle f(\vec{x})^{\star} f(\vec{x} + \vec{r}) \rangle.\]
Assuming translation invariance (i.e. homogeneity) we drop $\vec{x}$ as a dependency and write $\xi(\vec{r})$, as a function of $\vec{r}$.

\subsection{Power spectrum}
We can express the correlation function in terms of the Fourier components
$\hat{f}(\vec{k})$ of $f(\vec{x})$, using the following conventions
\begin{align}
    \mathcal{F}[f] = \hat{f}(\vec{k}) &= \int f(\vec{x}) e^{-i\vec{k}\cdot\vec{x}} \d\vec{x}\\
    \mathcal{F}^{-1} \lbrack\hat{f}\rbrack = f(\vec{x}) &= \int \hat{f}(\vec{k})
    e^{i\vec{k}\cdot\vec{x}} \frac{\d\vec{k}}{{(2\pi)}^3}.
\end{align}
Because of translation invariance, we might as well compute the Fourier transform of $\xi(\vec{x},\vec{x} + \vec{r})$ at the origin $\vec{x} = 0$. Swapping integration and averaging gives
\begin{align}
\xi(\vec{r}) &= \left<
    \int{\hat{f}(\vec{k})}^{\star}
    \frac{\d\vec{k}}{{(2\pi)}^3}
    \int\hat{f}(\vec{k'})e^{-i\vec{k'}\cdot\vec{r}}
    \frac{\d\vec{k'}}{{(2\pi)}^3} \right> \\
&= \int\int\left< {\hat{f}(\vec{k})}^{\star} \hat{f}(\vec{k'})\right>
%    e^{-i(\vec{k'}-\vec{k})\cdot\vec{x}}
    \frac{\d\vec{k}}{{(2\pi)}^3}
    e^{-i\vec{k'}\cdot\vec{r}}
    \frac{\d\vec{k'}}{{(2\pi)}^3}.
    \label{eq:tpcfhw}
\end{align}
In Fourier space we can express the translation operation as a phase-shift,
\begin{equation}
    \mathcal{F}[f(\vec{x} - \vec{r})] = \hat{f}(\vec{k}) e^{-i \vec{k}
    \cdot \vec{r}},
\end{equation}
which, again using invariance under translation, allows us to state
\begin{equation}
    \left<{\hat{f}(\vec{k})}^{\star}\hat{f}(\vec{k'}) \right> = \left<
    {\hat{f}(\vec{k})}^{\star}\hat{f}(\vec{k'}) \right> e^{i(\vec{k} -
    \vec{k'}) \cdot \vec{r}}\quad \forall \vec{r} \in \mathbb{R}^3.
\end{equation}
This can only be true when $\vec{k} = \vec{k'}$, and we can write
\begin{equation}
    \left< {\hat{f}(\vec{k})}^{\star}\hat{f}(\vec{k'}) \right> :=
    {(2\pi)}^3 \mathcal{P}(\vec{k})\delta(\vec{k} - \vec{k'}),
    \label{eq:powerdef}
\end{equation}
defining $\mathcal{P}(k)$ as the \emph{power spectrum}. Inserting this into Equation~\ref{eq:tpcfhw}, we see that
\begin{equation}
    \xi(\vec{r}) = \int \mathcal{P}(\vec{k}) e^{i\vec{k}\cdot\vec{r}} \frac{\d^3
    \vec{k}}{{(2\pi)}^3}.
\end{equation}

Knowing that the power spectrum contains all information needed to describe a (translation invariant) Gaussian random field, we learn that its Fourier phases are \emph{uncorrelated}. This means that, in order to generate random fields on a computer, it is most natural to do this in Fourier space. Following the property that linear combinations of Gaussian variables are still Gaussian, we may conclude that the real and imaginary parts of the Fourier coefficients will each have a Gaussian distribution with $\sigma^2 = {(2\pi)}^3/2 \mathcal{P}(\vec{k})$.


\subsection{The CDM power-spectrum}
\begin{SCfigure}[1.0]
    \includegraphics[width=0.5\textwidth]{figures/cdmps}
    \caption{The CDM power-spectrum. This is the power-spectrum $\mathcal{P}(k)$ of density fluctuations for pure cold dark matter. On the large scale end, it shows a behaviour tending towards $\mathcal{P}(k) \sim k$, while the small scale fluctuations have damped to a spectrum of $\mathcal{P}(k) \sim k^{-3}$.} \label{fig:cdm-power-spectrum}
\end{SCfigure}

The primordeal density perturbations have been measured in the CMB to have followed a power-spectrum $\mathcal{P}(k) = k^{n_s}$, where $n_s = 0.96$ \citep{Planck2013-cosmo}. This is very close the \emph{scale-free} Harisson-Zeldovich spectrum. As even massive particles are still behaving relativistically, the perturbations that are within a horizon distance will be damped. When the dark matter cools down to non-relativistic speeds, structure may start to form. This change in the power-spectrum is expressed in the \emph{transfer function} $T_0(k)$, such that
\[\mathcal{P}(k) = A k^{n_s} {T_0(k)}^2.\]
Then the form of $T_0$ can be found numerically. Some authors \citep{bbks,Eisenstein1998,Eisenstein1999} have given fitting formulas. We will be using the transfer function of \citet{Eisenstein1998}, neglecting baryon accoustic oscillations.
\begin{eqnarray}
    L_0(q) &=& \log(2 e + 1.8 q) \\
    C_0(q) &=& 14.2 + \frac{731}{1 + 62.5 q} \\
    T_0(q) &=& \frac{L_0}{L_0 + C_0 q^2},
\end{eqnarray}
where $q$ is a rescaling of the Fourier modes to place the \emph{knee} of the power-spectrum at $k \sim k_{eq}$,
\[q = \frac{k}{h {\rm Mpc^{-1}}} \frac{\Theta^2_{2.7}}{\Omega_0 h},\]
$\Theta_{2.7}$ being the temperature of the microwave background $T_{\rm CMB}/2.7$. At large scales this still behaves as the, close to Harisson-Zeldovich, primordial spectrum. At small scales however, it converges to a behaviour of $\mathcal{P}(k) \propto k^{-3}$, see Figure~\ref{fig:cdm-power-spectrum}.

\subsection{Normalisation}
In the following sections we will see that the Zeldovich Approximation predicts that the time of collapse for perturbations at a certain scale depends inversely on their density amplitude. Given a shape of the power-spectrum we may use this fact to normalise $P(k)$ such that at $z=0$, structures of a prefered scale are on the verge of collapse. Traditionally the chosen scaling is applied using a spherical top-hat filter of radius $8\ \Mpc$. It was long believed that density perturbations on this scale have a standard deviation $\sigma_8$ of around unity. The latest measurements from Planck \citep{Planck2013-cosmo} give $\sigma_8 = 0.83 \pm 0.02$ of density perturbations linearly extrapolated to current epoch.

Now, given a density field $f$ we may filter this field with a spherical top-hat function with radius of $8\ \Mpc$ by means of a convolution $f_R = f \ast W_{\rm th}$. Then $\sigma_8^2 \equiv \langle f_8^{\star}f_8 \rangle$ can be expressed in Fourier space as
\[\sigma_R^2 = \int \mathcal{P}(\vec{k}) \hat{W}_{\rm th}^2(\vec{k}) \frac{\d^3 \vec{k}}{{(2\pi)}^3}.\]
Because all terms in the integral only depend on $|\vec{k}|$, we may rewrite this as
\[\sigma_R^2 = \int_0^{\infty} \mathcal{P}(k)\ \hat{W}_{\rm th}^2(k R)\ k^2 \frac{\d k}{2 \pi^2},\]
where the Fourier transform of the top-hat window function is given by
\[\hat{W}_{th}(y) = \frac{3}{y^3}\left(\sin y - y \cos y\right).\]

\subsection{On amplitudes and phases}
\begin{FPfigure}
  \includegraphics[width=\textwidth]{figures/hammock}
  \caption{Phase information. We swapped the Fourier phase information of the top images to obtain the bottom two images. In the middle we see the complex Fourier phases belonging to the top images illustrated in hue and saturation. The hue encodes the Fourier phases going from red through yellow, green, cyan, blue and purple back to red, while the saturation of the colour gives the amplitudes.}\label{fig:hammock}
\end{FPfigure}
The power spectrum is applied in many situations, even when the quantities concerned are not Gaussian. It should be noted that although the power spectrum can remain a potent tool to check the physicality of results, it will never suffice to give a statistical description of non-Gaussian phenomena. Yet, the power spectrum (or the equivalent two-point correlation function) is still much used to distinguish between different models of structure formation. Notably, nearly half of Peebles' seminal book is geared towards understanding correlation functions.

One big problem with describing the structures in the cosmic web, is the inherent non-linearity of the formed structures. The resulting density fields are nowhere near Gaussian. We show what this means by a simple example in Figure \ref{fig:hammock}. We take two fields, say $f_{\rm grf}$ and $f_{\rm cat}$. One is a proper \ac{GRF}, with a power spectrum of $P(\vec{k}) = |\vec{k}|^{-2}$. The other is a random picture from the internet.

We take the Fourier transform of both. The random field shows uncorrelated phase information and amplitudes falling off by the given power spectrum. The Fourier information of the cat is quite different. First of all, the image is far from isotropic, which is most prominently seen in the Fourier modes running perpendicular to direction of the sharp hammock features at the top of the image in real space.
Furthermore the phase information is correlated, showing a pattern not unlike a finger print. Where the amplitude is higher we also seem to find a greater phase correlation.

Now we swap the phases and amplitudes between both images and see what information is preserved in both cases. Then we see that all the information on the geometry of objects is stored in the phases. We can still recognise the image of the cat where we assigned the amplitudes form the \ac{GRF}. The other way around, we may notice the relatively large power caused by the top part of the hammock, (We have been lenient here, copying the entire two-dimensional power spectrum!) however all the relevant geometrical information that made us recognise the image is gone. In fact if we squint our eyes, we can still see the contours of the original \ac{GRF}.

\clearpage
\section{Eulerian and Lagrangian space}\label{section:eureianlagrangian}
In systems of many particles there are two basic approaches to compute the dynamics.  One is to put a grid on \emph{configuration space}, the space that we experience from day to day as the world we live in, also called \emph{Eulerian space}. This is the space in which we know the laws of physics. Particles living in this space change their location as they move. To each of these locations we can assign properties like pressure, temperature or bulk velocity. This last property poses a problem. When two particles occupy the same location, we cannot assign a unique velocity to this location.
In this case it makes more sense to follow the particles around, assigning each of them their own velocity. This is knows as the Lagrangian view point. It is this approach that is especially usefull in the study of structure formation.  Dark matter particles do not feel pressure from their peers, and will happily cross each others paths without noticing. Because the coordinates `travel with the particles', the Lagrangian viewpoint adapts the spatial resolution automatically to increase where the density is higher.

\subsection{Lagrangian derivatives}
We write down the transformation from Lagrangian to Eulerian space in comoving coordinates, or the Lagrangian map $\mathcal{L}_t: \vec{q} \mapsto \vec{x}$ as,
\begin{equation}
\vec{x}(\vec{q}, t_0) = \vec{q} + \vec{s}(\vec{q}, t_0) = \vec{q} +
\int_0^{t_0} \frac{\vec{v}(\vec{q}, t)}{a(t)} \d t,
\label{eq:lagrmap}
\end{equation}
where $\vec{x}$ is the comoving (Eulerian) position and $\vec{q} = \vec{x}(0)$ the Lagrangian position.

When transforming equations from Eulerian to Lagrangian form we need to take great care not to mix up derivatives with respect to these systems. Given some property $f(\vec{x}, t)$, in classical notation, we use the \emph{partial derivative} $\partial_t f$ to mean the change of $f$ in time, without regard to the dependence on time of $\vec{x}$. While the \emph{convective derivative} $\d_t f$ is to mean the change of $f$ while keeping to the same particle $\vec{q}$. In physical coordinates, the full derivative is then written as
\begin{equation}
\d_t f = \partial_t f + (\vec{v} \cdot \grad) f,
\end{equation}
This notation can be confusing. It suggests that the concepts of total and partial derivatives are somehow fundamentally different, while the only thing that changes is the choice of coordinates. We could choose to express the same quantity in Lagrangian space $f(\vec{x}, t) = \tilde{f}(\vec{q}, t)$.  This describes the same quantity, but the \emph{function} is different. We will use a different notation for the derivatives
\begin{align}
\d_t f & \to \partial_t f \big|_q \\
\partial_t f & \to \partial_t f \big|_x,
\end{align}
where the $|_{?}$ denotes the quantity that is kept constant. Using Equation~\ref{eq:lagrmap}, we can write the expression $\partial_t f |_q$ in terms of $\partial_t f |_x$. Let $\mathcal{L}_t: \vec{q} \mapsto \vec{x}$ be the Lagrangian map. Then
\begin{align}
\partial_t \tilde{f}(\vec{q}, t) \big|_q & = \partial_t f(\vec{x}, t) \big|_q =
\partial_t f (\mathcal{L}(\vec{q}, t), t) \big|_q \\
& = \partial_t \mathcal{L}(\vec{q}, t) \big|_q \partial_x f(\vec{x},t) \big|_t +
\partial_t f(\vec{x}, t)\big|_x
\end{align}
Now that all partial derivatives are matched with the arguments to the functions, we can write this in shorter notation, using Equation~\ref{eq:lagrmap},
\begin{equation}
\partial_t f \big|_q = \partial_t f \big|_x + \frac{1}{a} (\vec{v} \cdot \gradx) f
\end{equation}
The maps between Eulerian and Lagrangian space will form a crucial part in this thesis. Lagrangian space gives a representation of mass, and Eulerian that of volume. In chapter 2 and 3 we will see that we can relate these representations as being \emph{dual} quantities in a common Euler-Lagrange \emph{phase-space}. \\

\begin{SCfigure}[2.0]
    \includegraphics[width=0.33\textwidth]{figures/parallel}
    \caption{Lagrangian mass element. As the element deforms, density changes inversely proportional to the volume of the parallelogram spanned by the interval $[\vec{q}, \vec{q} + \d\vec{q}]$.} \label{fig:parallel}
\end{SCfigure}

\subsection{Density}
We can derive an interesting expression for the density in Lagrangian space, starting with the Lagrangian notion of mass conservation,
\begin{equation}
\rho_u \d \vec{q} = \rho \d \vec{x}.
\end{equation}
This states that the mass within a Lagrangian volume is equal to the same mass mapped and deformed to Eulerian coordinates, in the process either compressing or expanding the parcel of matter. If nothing happens to the clustering of the particles in comoving space, both the average density of the universe $\rho_u$ and the physical density of the system $\rho$ drop as $a^{-3}$. From this equation we can continue to compute the density
\begin{equation}
\frac{\rho}{\rho_u} = \delta + 1 = \left|\det\left(\frac{\d \vec{x}}{\d \vec{q}}\right)\right|^{-1}.
\end{equation}
Introducing the deformation tensor $d_{ij} = -\partial s_{i}/\partial q_{j}$,
\begin{equation}
\frac{\partial x_{i}}{\partial q_{j}} = I_{ij} - d_{ij},
\end{equation}
and diagonalising, we can write the density in terms of the eigenvalues of $d_{ij}$
\begin{shaded*}
\begin{equation}
\rho(\vec{q}, t) = \frac{\rho_u(t)}{\left|\left(1 - \lambda_1(\vec{q}, t)\right)
\left(1 - \lambda_2(\vec{q}, t)\right) \left(1 - \lambda_3(\vec{q},
t)\right)\right|}.
\label{eq:density}
\end{equation}
\end{shaded*}
We use the convention of sorting the eigenvalues, $\lambda_1 \ge \lambda_2 \ge \lambda_3$.

This expression has singularities whenever one of the eigenvalues approaches unity. Initially the Lagrangian map $\mathcal{L}_t: \vec{q} \mapsto \vec{x}$ will be \emph{bijective}, but as soon as the first particles went through a singularity, that is, having one of the eigenvalues $\lambda_i$ approach unity, the mapping develops a fold and is no longer invertible. To transform this density to an expression in Eulerian space, we need to sum over all the particles $\vec{q}$ that occupy a location $\vec{x}$,
\begin{equation}
\rho(\vec{x}, t) = \sum_{\vec{x} = \mathcal{L}(\vec{q^{\star}}, t)} \rho(\vec{q^{\star}}, t).
\label{eq:density-stack}
\end{equation}

\subsection{Multi-streaming}\label{section:ch1-catastrophe}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/cusp-proj}
    \caption{Multi-streaming. The first figure shows a 2-d example of a Lagrangian map where multi-streaming occurs. Two Eulerian directions are shown as the $x_1, x_2$ coordinates. The third dimension takes us into Lagrangian space. Moving down the surface in the $x_2$ direction, it develops a \emph{fold}; the point at which this happens is called the \emph{cusp}. On the right we can see the projection of this surface down to Eulerian space. We may observe that between two folds there is a three-stream region.} \label{fig:simple-cusp}
\end{figure}

In Figure~\ref{fig:simple-cusp} we show what a singularity in Equation~\ref{eq:density} may look like. Is this particular case the Lagrangian map has the form
\begin{align}
    x_1 &= q_1 q_2 - a q_1^3 \\
    x_2 &= q_2.
\end{align}
To understand the geometry of this situation it is helpful to visualise the Lagrangian map as a surface embedded in a higher dimensional \emph{phase-space}. We call this surface the \emph{Lagrangian sub-manifold}. To obtain the density from this description, we need to project this surface down to Eulerian space. This projection introduces some major analytical and numerical chalanges in dealing with the Lagrangian description of structure formation.

At the points where the projection is singular, density becomes formally \emph{infinite}. Consequences of this problem depend on the underlying dark-matter physics, and how closely the idealised model resembles reality. The $\Lambda$CDM scenario predicts a bottom up formation scenario. As the power spectrum approaches $\propto k^{-3}$ behaviour moving to smaller and smaller scales, we expect overdensities to collapse on many scales at the same time. Do we expect planet sized dark-matter haloes at some point? How small can we go? This prediction must break down at some point.

% Also, the projection introduces \emph{non-locality}. From the moment of shell-crossing onward, the behaviour of a particle does not only depend on its direct initial surroundings, but also on the neighbourhoods of particles that map to the same Eulerian location. At this point there will be no simple recipe to deal with the statistics linking properties of initial conditions with properties at $z=0$. In part this problem is dealt with as the cloud-in-cloud problem in the \emph{excursion set formalism}. Also we will see that the \emph{adhesion model} embraces non-local geometry to model the     behaviour of structure formation.


\clearpage
\section{Zeeman machine}\label{section:zeemanmachine}
\begin{figure}[t]
\includegraphics[width=\textwidth]{figures/zeeman}
\caption{Zeeman's catastrophe machine. This is an example of a relatively simple construction that exhibits catastrophic behaviour. The red points (on the left, and at the center of the disc) are fixed. Elastics are spanned between the red point on the left, the pin on the rim of the disc, and between the pin and the pointer on the right. As we move the pointer on the right, the pin on the rim of the disc will change its angle $\theta$. The checker board pattern follows lines of constant $\theta$ (for both the minima and maxima in the potential); tracking the lines in the checker board pattern corresponding to the minima in the potential with the pointer will preserve the position of the disc. This pattern was created by multiplying the signs of the first derivative of the potential for a score of angle positions $\theta$ on a regular interval. In the rhomboid area we see the pattern crossing itself; the solution for an optimal $\theta$ is multivalued there. If the pointer, moving from the rhomboid, crosses the edge of the rhomboid one of the multiple solutions disappears, causing the catastrophic behaviour of the disc.}
\label{fig:zeeman}
\end{figure}

Catastrophe theory describes how discrete phenomena or \emph{discontinuous behaviour} can emerge from systems which are described by a set of smoothly varying parameters. An example of a machine that exhibits catastrophic behaviour is \emph{Zeeman's catastrophe machine} \citep[described in][Chapter 5]{Poston1996}. It is a two-dimensional machine, consisting of a rotating disc with a pin near its edge, and two elastics, one spanned from a fixed point off the disc to the pin on the disc, the other from the pin to a moveable pointer (see Figure~\ref{fig:zeeman}).

As we move the pointer around, the elastics will force the disc into a position that minimises the tension on the elastics. In this process the disc may enter a state that could be described as a \emph{false vacuum}. Changing the location of the pointer, smoothly transforms a global minimum of the potential to a local minimum.

Going further this minimum changes from a stable equilibrium to an unstable one. The slightest nudge will then make the disc jump to the new global minimum. In Figure~\ref{fig:zeeman} we have plotted the behaviour of this simple machine as a checker board pattern. There is a rhomboid area on the right of this figure, where the disc can have multiple stable positions. Which position the disc attains if the pointer is situated in the rhomboid, depends on the previous history. The edge of the rhomboid is where the jumpy behaviour may occur. This edge is where the singularities of this system reside. In the Figure~\ref{fig:zeeman} we see the edge appearing as a caustic.
\end{subappendices}

% vim: set aw ts=4 sw=4 tw=0 wrap lbr:expandtab
